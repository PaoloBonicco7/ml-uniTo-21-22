{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEKy3XmT7H-U"
      },
      "source": [
        "# # Classifiers introduction\n",
        "\n",
        "In the following program we introduce the basic steps of classification of a dataset in a matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btrnWCpJ7H-W"
      },
      "source": [
        "Import the package for learning and modeling trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sx9T_yz7H-f"
      },
      "source": [
        "Define the matrix containing the data (one example per row)\n",
        "and the vector containing the corresponding target value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]] #TRAINING SET (EXAMPLES)\n",
        "Y = [1, 0, 0, 0, 1, 1] #TRAINING SET (LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLgMBwpZ7H-n"
      },
      "source": [
        "Declare the classification model you want to use and then fit the model to the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUtyP-xO7H-t"
      },
      "source": [
        "Predict the target value (and print it) for the passed data, using the fitted model currently in clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(clf.predict([[0, 1, 1]])) #esempio ignoto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(clf.predict([[1, 0, 1],[0, 0, 1]])) #due esempi noti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizziamo come è fatto l'albero che è stato creato con il fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'graphviz'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/1h/nl4lppwj1hgdkrq7c63vgdpr0000gn/T/ipykernel_6863/2492329565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#crea uno script in DOT, linguaggio usato per descrivere grafici\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
          ]
        }
      ],
      "source": [
        "import graphviz \n",
        "dot_data = tree.export_graphviz(clf, out_file=None)  #crea uno script in DOT, linguaggio usato per descrivere grafici\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oCMYXsO7H--"
      },
      "source": [
        "In the following we start using a dataset (from UCI Machine Learning repository)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzZnLA4N7H_D"
      },
      "source": [
        "# Declare the type of prediction model and the working criteria for the model induction algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1}) \n",
        "#random_state: per fissare la randomizzazione e quindi ottenere un comportamento deterministico per la randomizzazione\n",
        "#min_samples_leaf: una foglia dovrà contenere ALMENO 5 esempi per esistere (cosi si evita overfitting)\n",
        "#class_weight: indice:peso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqH8H1UD7H_H"
      },
      "source": [
        "# Split the dataset in training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random permutation on indexes: DIVIDERÀ TRAINING E TEST SET\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "indices = np.random.permutation(len(iris.data)) #INDICI DA 0 A 149 (len(iris.data)) PERMUTATI RANDOM\n",
        "\n",
        "# We now decide to keep the last 10 indices for test set, the remaining for the training set\n",
        "indices_training=indices[:-10] #INDICI TRAINING SET: PRIMI 140 ELEMENTI DEL VETTORE RANDOM\n",
        "indices_test=indices[-10:] #INDICI TEST SET: ULTIMI 10 ELEMENTI DEL VETTORE RANDOM\n",
        "\n",
        "iris_X_train = iris.data[indices_training]\n",
        "iris_y_train = iris.target[indices_training]\n",
        "iris_X_test  = iris.data[indices_test]\n",
        "iris_y_test  = iris.target[indices_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNbXUYI07H_K"
      },
      "source": [
        "# Fit the learning model on training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit the model to the training data\n",
        "clf = clf.fit(iris_X_train, iris_y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPdjlWZB7H_P"
      },
      "source": [
        "# Obtain predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply fitted model \"clf\" to the test set \n",
        "predicted_y_test = clf.predict(iris_X_test)\n",
        "\n",
        "# print the predictions (class numbers associated to classes names in target names)\n",
        "print(\"Predictions:\")\n",
        "print(predicted_y_test)\n",
        "print(\"True classes:\")\n",
        "print(iris_y_test) \n",
        "print(iris.target_names)\n",
        "\n",
        "#QUINDI C'è STATO UN SOLO ERRORE, SUL SECONDO ESEMPIO, PREDETTO COME VIRGINICA MA E' IN REALTA' VERSICOLOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TZyuYLx7H_T"
      },
      "source": [
        "Print the index of the test instances and the corresponding predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print the corresponding instances indexes and class names \n",
        "for i in range(len(iris_y_test)): \n",
        "    print(\"Instance # \"+str(indices_test[i])+\": \")\n",
        "    print(\"Predicted: \"+iris.target_names[predicted_y_test[i]]+\"\\t True: \"+iris.target_names[iris_y_test[i]]+\" [\"+\n",
        "                        (\"OK\" if iris.target_names[predicted_y_test[i]]==iris.target_names[iris_y_test[i]] else \"NOT OK\")\n",
        "          +\"]\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj3hYAqQ7H_W"
      },
      "source": [
        "# Look at the specific examples (TEST SET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(iris_y_test)): \n",
        "    print(\"Instance # \"+str(indices_test)+\": \")\n",
        "    s=\"\"\n",
        "    for j in range(len(iris.feature_names)):\n",
        "        s=s+iris.feature_names[j]+\"=\"+str(iris_X_test[i][j])\n",
        "        if (j<len(iris.feature_names)-1): s=s+\", \"\n",
        "    print(s)\n",
        "    print(\"Predicted: \"+iris.target_names[predicted_y_test[i]]+\"\\t True: \"+iris.target_names[iris_y_test[i]]+\"                                              \\t[\"+\n",
        "                        (\"OK\" if iris.target_names[predicted_y_test[i]]==iris.target_names[iris_y_test[i]] else \"NOT OK\")\n",
        "          +\"]\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9zos0wm7H_Z"
      },
      "source": [
        "# Obtain model performance results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print some metrics results\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "acc_score = accuracy_score(iris_y_test, predicted_y_test)\n",
        "print(\"Accuracy score: \"+ str(acc_score))\n",
        "AVG = 'macro'\n",
        "f1=f1_score(iris_y_test, predicted_y_test, average=AVG) #F1 SCORE = BALANCED F-SCORE (MEDIA ARMONICA) = 2 * (precision * recall) / (precision + recal)\n",
        "print(\"F1 score (\"+AVG+\"): \"+str(f1))                                 \n",
        "#'macro' = MEDIA NON PESATA DELLO SCORE SU OGNI CLASSE (OVVERO: ESEMPI PESATI TUTTI ALLO STESSO MODO)  --> MEGLIO, SOPRATTUTTO SE LE DISTRIBUZIONI SONO DIFFERENTI!\n",
        "#'micro' = MEDIA CALCOLATA GLOBALMENTE SUI TP, FN, FP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNO0xcGq7H_c"
      },
      "source": [
        "# Use Cross Validation (5 FOLDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmt414Lm7H_c",
        "outputId": "da09947d-d490-4717-cd7c-c2c544a0d261"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score # will be used to separate training and test\n",
        "iris = load_iris()\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})\n",
        "clf = clf.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-rd5Dq77H_e",
        "outputId": "0c1f75ee-948a-4184-a9f1-7c10a7747aeb"
      },
      "outputs": [],
      "source": [
        "# computes F1-score\n",
        "f1_scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='f1_'+AVG)\n",
        "print(AVG)\n",
        "print(f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcnegz_A7H_h"
      },
      "source": [
        "# Show the resulting tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM67N3vc7H_k"
      },
      "source": [
        "## 2. Generate a picture here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prgjDOka7H_l",
        "outputId": "2263ea70-f260-4933-80f3-63bc23d00ace"
      },
      "outputs": [],
      "source": [
        "print(list(iris.feature_names))\n",
        "print(list(iris.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr32sBno7H_n",
        "outputId": "6f741234-c217-448b-b711-ed46e541e862"
      },
      "outputs": [],
      "source": [
        "dot_data = tree.export_graphviz(clf, out_file=None, \n",
        "                         feature_names=iris.feature_names, \n",
        "                         class_names=iris.target_names, \n",
        "                         filled=True, rounded=True,  \n",
        "                         special_characters=True)  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph\n",
        "\n",
        "#NODI BIANCHI: ENTROPIA MOLTO ALTA (NON HOMOGENEOUS)\n",
        "#NODI ARANCIONI: SETOSA\n",
        "#NODI VERDI: VERSICOLOR\n",
        "#NODI VIOLA: VIRGINICA\n",
        "\n",
        "#IL COLORE E' PIU DENSO --> L'ENTROPIA E' MINORE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90GvIMjgeUue"
      },
      "source": [
        "# Your work: what you have to do\n",
        "Modify the given Jupyter notebook on decision trees on Iris data and perform the following tasks:\n",
        "\n",
        "1. get an artificial inflation of some class in the training set by a given factor: 10 (weigh more the classes virginica e versicolor which are more difficult to discriminate). Learn the tree in these conditions.\n",
        "    1.b) modify the weight of some classes (set to 10 the weights for misclassification between virginica into versicolor and vice              versa) and learn the tree in these conditions. You should obtain similar results as for step 1.\n",
        "2. learn trees but try to avoid overfitting (by improving the error on the test set) tuning the hyper-parameters on: the minimum number of samples per leaf, max depth of the tree, min_impurity_decrease parameters, max leaf nodes, etc.\n",
        "3. build the confusion matrix of the created tree models on the test set and show them. \n",
        "4. build the ROC curves (or coverage curves in coverage space) and plot them for each tree model you have created: for each model you have to build three curves, one for each class, considered in turn as the positive class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. ARTIFICIAL INFLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TESTSET_SIZE = 20\n",
        "INFLATION_FACTOR = 20   #1A\n",
        "WEIGHT_FACTOR = 20      #1B\n",
        "AVG = 'macro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn import tree \n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# creo il classificatore (1a)\n",
        "clf_1a = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1}) \n",
        "# creo il classificatore (1b) coi pesi ricalibrati per versicolor, virginica\n",
        "clf_1b = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:WEIGHT_FACTOR,2:WEIGHT_FACTOR}) \n",
        "                                                                                                        #0:setosa, 1:versicolor, 2:virginica\n",
        "\n",
        "# creo il dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "for i, n in enumerate(iris.target_names):\n",
        "    print(i, \"-->\", n)\n",
        "\n",
        "# divido in trainingset e testset con random permutation, (1a) effettuo art.inflation sul training set x10 per versicolor, virginica\n",
        "np.random.seed(0)\n",
        "indices = np.random.permutation(len(X))\n",
        "indices_train = indices[:-TESTSET_SIZE]\n",
        "indices_test = indices[-TESTSET_SIZE:]\n",
        "X_train = X[indices_train]\n",
        "y_train = y[indices_train]\n",
        "X_test  = X[indices_test]\n",
        "y_test  = y[indices_test]\n",
        "X_train_inflated = X_train\n",
        "y_train_inflated = y_train\n",
        "for i in range(len(X_train)):\n",
        "    if y_train[i] == 1 or y_train[i] == 2:\n",
        "        for j in range(INFLATION_FACTOR):\n",
        "            X_train_inflated = np.append(X_train_inflated, [X_train[i]], 0)\n",
        "            y_train_inflated = np.append(y_train_inflated, [y_train[i]], 0)\n",
        "print(\"TRAINING SET SHAPE: X\", X_train.shape, \"y\", y_train.shape)\n",
        "print(\"TRAINING SET SHAPE (WITH INFLATION x\"+str(INFLATION_FACTOR)+\"): X\", X_train_inflated.shape, \"y\", y_train_inflated.shape)\n",
        "print(\"TEST SET SIZE:\", TESTSET_SIZE)\n",
        "\n",
        "# apprendimento su trainingset\n",
        "clf_1a.fit(X_train_inflated, y_train_inflated)\n",
        "clf_1b.fit(X_train, y_train)\n",
        "\n",
        "# evaluation su testset\n",
        "y_test_pred_1a = clf_1a.predict(X_test)\n",
        "y_test_pred_1b = clf_1b.predict(X_test)\n",
        "print(\"Predictions on test set:\")\n",
        "print(y_test_pred_1a, \"(1A)\")\n",
        "print(y_test_pred_1b, \"(1B)\")\n",
        "print(\"True classes:\")\n",
        "print(y_test) \n",
        "print(\"Errors (1A):\")\n",
        "print(np.array([0 if a==b else 1 for (a,b) in list(zip(y_test_pred_1a,y_test))]))\n",
        "print(\"Errors (1B):\")\n",
        "print(np.array([0 if a==b else 1 for (a,b) in list(zip(y_test_pred_1b,y_test))]))\n",
        "\n",
        "# accuracy e f1 score\n",
        "acc_score_1a = accuracy_score(y_test, y_test_pred_1a)\n",
        "acc_score_1b = accuracy_score(y_test, y_test_pred_1b)\n",
        "print(\"Accuracy score (1A): \", acc_score_1a)\n",
        "print(\"Accuracy score (1B): \", acc_score_1b)\n",
        "f1_score_1a = f1_score(y_test, y_test_pred_1a, average=AVG)\n",
        "f1_score_1b = f1_score(y_test, y_test_pred_1b, average=AVG)\n",
        "print(\"F1 score (1A) (\"+AVG+\"): \", f1_score_1a)\n",
        "print(\"F1 score (1B) (\"+AVG+\"): \", f1_score_1b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2,3. TUNE HYPER PARAMETERS TO PRODUCE DIFFERENT CLFs, BUILD CONFUSION MATRIXes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WEIGHT_FACTOR_SETOSA = 1\n",
        "WEIGHT_FACTOR_VERSICOLOR = 10\n",
        "WEIGHT_FACTOR_VIRGINICA = 10\n",
        "MIN_SAMPLES_LEAF = 5\n",
        "MAX_LEAVES = 10\n",
        "MAX_DEPTH = 10\n",
        "MIN_IMPURITY = 0\n",
        "CRITERION = 'entropy' #'gini'\n",
        "AVG = 'macro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "# confusion matrix function\n",
        "def draw_confusion_matrix(class_names,y_test,y_test_pred,clf_label):\n",
        "    cf = confusion_matrix(y_test,y_test_pred)\n",
        "    plt.figure()\n",
        "    plt.imshow(cf, cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix \\\"\"+clf_label+\"\\\"\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    thresh = cf.max() / 2.0\n",
        "    for i, j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
        "        plt.text(j, i, cf[i, j],horizontalalignment=\"center\",color=\"white\" if cf[i, j] > thresh else \"black\",)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "#tune hyper parameters to produce different clf\n",
        "clf_2 = tree.DecisionTreeClassifier(criterion=CRITERION,random_state=0,min_samples_leaf=MIN_SAMPLES_LEAF,max_depth=MAX_DEPTH,class_weight={0:WEIGHT_FACTOR_SETOSA,1:WEIGHT_FACTOR_VERSICOLOR,2:WEIGHT_FACTOR_VIRGINICA},max_leaf_nodes=MAX_LEAVES,min_impurity_decrease=MIN_IMPURITY) \n",
        "\n",
        "# apprendimento su trainingset\n",
        "clf_2.fit(X_train, y_train)\n",
        "\n",
        "# evaluation su testset\n",
        "y_test_pred_2 = clf_2.predict(X_test)\n",
        "print(\"Predictions on test set:\")\n",
        "print(y_test_pred_2, \"(2)\")\n",
        "print(\"True classes:\")\n",
        "print(y_test)\n",
        "print(\"Errors (2):\")\n",
        "print(np.array([0 if a==b else 1 for (a,b) in list(zip(y_test_pred_2,y_test))]))\n",
        "\n",
        "# accuracy e f1 score\n",
        "acc_score_2 = accuracy_score(y_test, y_test_pred_2)\n",
        "print(\"Accuracy score (2): \", acc_score_2)\n",
        "f1_score_2 = f1_score(y_test, y_test_pred_2, average=AVG)\n",
        "print(\"F1 score (2) (\"+AVG+\"): \", f1_score_2)\n",
        "\n",
        "# draw matrixes of all the models\n",
        "class_names = iris.target_names\n",
        "draw_confusion_matrix(class_names,y_test,y_test_pred_1a,\"1A\")\n",
        "draw_confusion_matrix(class_names,y_test,y_test_pred_1b,\"1B\")\n",
        "draw_confusion_matrix(class_names,y_test,y_test_pred_2,\"2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. BUILD ROC CURVEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def bin_convert(y,positive_label):\n",
        "    return np.where(y==positive_label,1,0)\n",
        "\n",
        "def draw_3_roc_curves(y_test_setosa,predicted_y_test_setosa,y_test_versicolor,predicted_y_test_versicolor,y_test_virginica,predicted_y_test_virginica,name):\n",
        "    plt.figure()\n",
        "    fpr, tpr, _ = roc_curve(y_test_setosa, predicted_y_test_setosa)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=\"Setosa (area = %0.2f)\" % roc_auc)\n",
        "    fpr, tpr, _ = roc_curve(y_test_versicolor, predicted_y_test_versicolor)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=\"Versicolor (area = %0.2f)\" % roc_auc)\n",
        "    fpr, tpr, _ = roc_curve(y_test_virginica, predicted_y_test_virginica)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=\"Virginica (area = %0.2f)\" % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], \"k\")\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC clf \"+name)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def build_roc_plot_3_classes_binary(y_test,predicted_y_test,name):\n",
        "    y_test_setosa = bin_convert(y_test,0)\n",
        "    predicted_y_test_setosa = bin_convert(predicted_y_test,0)\n",
        "    y_test_versicolor = bin_convert(y_test,1)\n",
        "    predicted_y_test_versicolor = bin_convert(predicted_y_test,1)\n",
        "    y_test_virginica = bin_convert(y_test,2)\n",
        "    predicted_y_test_virginica = bin_convert(predicted_y_test,2)\n",
        "\n",
        "    print(\"Predictions on test set:\",\"\\t\\t\", predicted_y_test,\"(\"+name+\")\")\n",
        "    print(\"True classes:\\t\\t\\t\\t\", y_test)\n",
        "    print(\"Errors (\"+name+\"):\\t\\t\\t\\t\", np.array([0 if a==b else 1 for (a,b) in list(zip(predicted_y_test,y_test))]))\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Predictions on test set - SETOSA\",\":\\t\", predicted_y_test_setosa,\"(\"+name+\")\")\n",
        "    print(\"True classes - SETOSA:\\t\\t\\t\", y_test_setosa)\n",
        "    print(\"Errors (\"+name+\"):\\t\\t\\t\\t\", np.array([0 if a==b else 1 for (a,b) in list(zip(predicted_y_test_setosa,y_test_setosa))]))\n",
        "    print()\n",
        "    print(\"Predictions on test set - VERSICOLOR\",\":\\t\", predicted_y_test_versicolor,\"(\"+name+\")\")\n",
        "    print(\"True classes - VERSICOLOR:\\t\\t\", y_test_versicolor)\n",
        "    print(\"Errors (\"+name+\"):\\t\\t\\t\\t\", np.array([0 if a==b else 1 for (a,b) in list(zip(predicted_y_test_versicolor,y_test_versicolor))]))\n",
        "    print()\n",
        "    print(\"Predictions on test set - VIRGINICA\",\":\\t\", predicted_y_test_virginica,\"(\"+name+\")\")\n",
        "    print(\"True classes - VIRGINICA:\\t\\t\", y_test_virginica)\n",
        "    print(\"Errors (\"+name+\"):\\t\\t\\t\\t\", np.array([0 if a==b else 1 for (a,b) in list(zip(predicted_y_test_virginica,y_test_virginica))]))\n",
        "\n",
        "    draw_3_roc_curves(y_test_setosa,predicted_y_test_setosa,y_test_versicolor,predicted_y_test_versicolor,y_test_virginica,predicted_y_test_virginica,name)\n",
        "\n",
        "build_roc_plot_3_classes_binary(y_test,y_test_pred_1a,\"1A\")\n",
        "build_roc_plot_3_classes_binary(y_test,y_test_pred_1b,\"1B\")\n",
        "build_roc_plot_3_classes_binary(y_test,y_test_pred_2,\"2\")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "classification_iris_aa_19_20.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
